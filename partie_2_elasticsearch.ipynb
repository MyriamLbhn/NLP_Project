{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocker les données avec Elastic Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\t\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from faker import Faker\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mise en place"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir installé l'extension Docker dans VS Code et s'être connecté, on suit les étapes suivantes\n",
    "\n",
    "a) Démarrer un container à partir de l’image docker.elastic.co/elasticsearch/elasticsearch:7.17.10  \n",
    "- en mode détaché -d\n",
    "- monter le volume /usr/share/elasticsearch/data en local\n",
    "- utiliser le mapping de port -p 9200:9200\n",
    "- utiliser la variable -e \"discovery.type=single-node\"\n",
    "- le nom du container sera --name elastic\n",
    "\n",
    "commande :   dans le répertoire de travail   \n",
    "`docker run -d -p 9200:9200 -e \"discovery.type=single-node\" -v $(pwd):/usr/share/elasticsearch/data --name elastic docker.elastic.co/elasticsearch/elasticsearch:7.17.10`\n",
    "\n",
    "docker run -d -p 9200:9200 -e \"discovery.type=single-node\" -v /home/apprenant/Documents/DevIA/Projet_DevIA/NLP_Project:/usr/share/elasticsearch/data --name elastic docker.elastic.co/elasticsearch/elasticsearch:7.17.10\n",
    "\n",
    "\n",
    "b) Visualiser les logs du container  \n",
    "\n",
    "commande :   \n",
    "`docker logs elastic`\n",
    " \n",
    "c) appeler la route racine “/”   \n",
    "\n",
    "commande :   \n",
    "`curl http://localhost:9200/`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mapper et importer les données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Poster le mapping d’un index nommé “notes”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il doit contenir :  \n",
    "\n",
    "- un champ “patient_lastname” qui est un term\n",
    "- un champ “patient_firstname” qui est un term\n",
    "- un champ “text”, texte analysé en standard\n",
    "- un champ “date”, qui est une date\n",
    "- un champ “patient_left”, qui est un booléen\n",
    "- un champ “emotion”, qui est un term\n",
    "- un champ ‘confidence” qui est un float  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour poster le mapping de l'index \"notes\" avec les champs spécifiés dans Elasticsearch, on utilise l'API REST d'Elasticsearch avec cURL :  \n",
    "\n",
    "Le conteneur Elasticsearch crée à l'étape précédente doit être en cours d'exécution.  \n",
    "\n",
    "Dans un terminal, il faut entrer la commande suivante pour poster le mapping de l'index \"notes\" :  \n",
    "\n",
    "```curl -X PUT \"http://localhost:9200/notes\" -H 'Content-Type: application/json' -d '\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"patient_lastname\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"patient_firstname\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"text\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"standard\"\n",
    "      },\n",
    "      \"date\": {\n",
    "        \"type\": \"date\"\n",
    "      },\n",
    "      \"patient_left\": {\n",
    "        \"type\": \"boolean\"\n",
    "      },\n",
    "      \"emotion\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"confidence\": {\n",
    "        \"type\": \"float\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "Cette commande envoie une requête PUT à l'URL http://localhost:9200/notes pour créer l'index \"notes\" avec le mapping spécifié. Chaque champ est défini avec son type approprié."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Alimenter l’index “notes” à l’aide du jeu de données et de la librairie Faker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Créer une instance de Faker\n",
    "# fake = Faker()\n",
    "\n",
    "# # Créer une instance d'Elasticsearch\n",
    "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "# # Générer des données et les indexer dans l'index \"notes\"\n",
    "# for _ in range(100):\n",
    "#     note = {\n",
    "#         \"patient_lastname\": fake.last_name(),\n",
    "#         \"patient_firstname\": fake.first_name(),\n",
    "#         \"text\": fake.text(),\n",
    "#         \"date\": fake.date_time_this_decade(),\n",
    "#         \"patient_left\": fake.boolean(),\n",
    "#         \"emotion\": fake.word(),\n",
    "#         \"confidence\": fake.pyfloat(left_digits=2, right_digits=2)\n",
    "#     }\n",
    "#     es.index(index=\"notes\", document=note)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise la librairie Faker pour générer des valeurs aléatoires pour chaque champ, puis il utilise le client Elasticsearch pour indexer les données dans l'index \"notes\". Vous pouvez ajuster le nombre de documents générés en modifiant la valeur de range(100) selon vos besoins."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les notes générées sont accessibles à l'URL suivante : http://localhost:9200/notes/_search  \n",
    "\n",
    "Pour accèder aux nombres de notes dans la BDD : http://localhost:9200/notes/_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de \"fakes\" notes générées : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une instance d'Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "# Effectuer une requête pour récupérer les notes\n",
    "result = es.search(index=\"notes\", size=3)  # Récupère les 3 premiers documents, ajustez la taille selon vos besoins\n",
    "\n",
    "# Parcourir les résultats et afficher les notes\n",
    "for hit in result['hits']['hits']:\n",
    "    note = hit['_source']\n",
    "    print(\"Patient Last Name:\", note['patient_lastname'])\n",
    "    print(\"Patient First Name:\", note['patient_firstname'])\n",
    "    print(\"Text:\", note['text'])\n",
    "    print(\"Date:\", note['date'])\n",
    "    print(\"Patient Left:\", note['patient_left'])\n",
    "    print(\"Emotion:\", note['emotion'])\n",
    "    print(\"Confidence:\", note['confidence'])\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mettez en place un pipeline utilisant le modèle TF-IDF que vous avez développé avec scikit-learn pour remplir les champs “emotion” et “confidence”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe **TextProcessor** est définie dans *preprocessing.py*, puis le pipeline est mis en place dans le fichier *tf-idf.py*, on obient alors le fichier pickle *nlp-pipeline.pkl* que l'on va importer et utiliser pour remplir les champs \"emotion\" et \"confidence\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remplis les champs de l'index comme suit :   \n",
    "\"patient_lastname\" : Faker  \n",
    "\"patient_firstname\" : Faker  \n",
    "\"text\": CSV  \n",
    "\"date\" : Faker  \n",
    "\"patient_left\" : Faker  \n",
    "\"emotion\" : Model  \n",
    "\"confidence\" : Model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexation terminée.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from elasticsearch import Elasticsearch\n",
    "from faker import Faker\n",
    "import csv\n",
    "from preprocessing import TextProcessor\n",
    "\n",
    "\n",
    "# Charger le pipeline pré-entraîné\n",
    "with open('pipeline/nlp-pipeline-linearsvc.pkl', 'rb') as f:\n",
    "    pipeline = pickle.load(f)\n",
    "\n",
    "# Connexion à Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "# Instanciation de Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Liste de patients\n",
    "patients = []\n",
    "for _ in range(200):\n",
    "    patients.append((fake.last_name(), fake.first_name()))\n",
    "\n",
    "# Chemin vers le fichier CSV\n",
    "csv_file = 'data/Emotion_final.csv'\n",
    "\n",
    "# Lecture du fichier CSV et indexation des données\n",
    "with open(csv_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for i, row in enumerate(reader):\n",
    "        # Sélectionner le texte de la ligne\n",
    "        text = row['Text']\n",
    "\n",
    "        emotion = pipeline.predict([text])[0]\n",
    "        decision = pipeline.decision_function([text])[0]\n",
    "        confidence = max(1 / (1 + np.exp(-decision)))\n",
    "        confidence = round(confidence, 2)\n",
    "\n",
    "        # Sélectionner le patient correspondant\n",
    "        patient_lastname, patient_firstname = patients[i % len(patients)]\n",
    "\n",
    "        # Créer le document à indexer\n",
    "        doc = {\n",
    "            'patient_lastname': patient_lastname,\n",
    "            'patient_firstname': patient_firstname,\n",
    "            'text': text,\n",
    "            'date': fake.date(),\n",
    "            'patient_left': fake.boolean(),\n",
    "            'emotion': emotion,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "        # Indexation du document dans Elasticsearch\n",
    "        es.index(index='notes', body=doc)\n",
    "\n",
    "print(\"Indexation terminée.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Requêtes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 En recherchant dans la base elastic search, aboutissez à un data frame permettant d’afficher la répartition des sentiments des textes pour un patient (nom/prénom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Sentiment, Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "# import pandas as pd\n",
    "\n",
    "# # Créer une instance d'Elasticsearch\n",
    "# es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "# # Paramètres du patient\n",
    "# patient_lastname = \"Campos\"\n",
    "# patient_firstname = \"Steven\"\n",
    "\n",
    "# # Requête Elasticsearch pour obtenir la répartition des sentiments pour le patient\n",
    "# query = {\n",
    "#   \"query\": {\n",
    "#     \"bool\": {\n",
    "#       \"must\": [\n",
    "#         {\"match\": {\"patient_lastname\": patient_lastname}},\n",
    "#         {\"match\": {\"patient_firstname\": patient_firstname}}\n",
    "#       ]\n",
    "#     }\n",
    "#   },\n",
    "#   \"aggs\": {\n",
    "#     \"sentiment_distribution\": {\n",
    "#       \"terms\": {\n",
    "#         \"field\": \"emotion.keyword\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# # Effectuer la requête\n",
    "# result = es.search(index=\"notes\", body=query, size=0)\n",
    "\n",
    "# # Récupérer les agrégations\n",
    "# aggregations = result['aggregations']['sentiment_distribution']['buckets']\n",
    "\n",
    "# # Créer le dataframe à partir des résultats\n",
    "# df = pd.DataFrame(aggregations, columns=['Sentiment', 'Count'])\n",
    "\n",
    "# # Afficher le dataframe\n",
    "# print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Élaborez une matrice de sentiments contradictoire (toujours en utilisant la base elastic search.    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut savoir parmi les documents classifiés comme *happy*, quel pourcentage contient le mot *sadness*. Puis quel pourcentage contient *fear*, etc.  Et cela, pour tous les sentiments.  \n",
    "On représente les résultats dans une HeatMap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Pour chacune des étapes du deuil (denial, anger, bargaining, depression, and acceptance) rechercher le nombre de text correspondants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à l’aide:  \n",
    "- d’une recherche pleine  \n",
    "- d’une fuzzy recherche.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Rechercher les textes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- qui doivent matcher l’expression “good day” (must)  \n",
    "- chez les patients encore en consultation (filter)  \n",
    "- qui contiennent si possible “to rest” (should)  \n",
    "- qui ne doivent pas avoir seuil de confiance inférieur à 0.5 s’il existe.   \n",
    "- Observer la répartition de ces résultats par sentiment  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effacer la BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une erreur s'est produite lors de la suppression des documents.\n",
      "{\"error\":{\"root_cause\":[{\"type\":\"index_not_found_exception\",\"reason\":\"no such index [notes]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\"notes\",\"index_uuid\":\"_na_\",\"index\":\"notes\"}],\"type\":\"index_not_found_exception\",\"reason\":\"no such index [notes]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\"notes\",\"index_uuid\":\"_na_\",\"index\":\"notes\"},\"status\":404}\n"
     ]
    }
   ],
   "source": [
    "# index = 'notes'  # Remplacez par le nom de votre index\n",
    "\n",
    "# # URL de l'API de suppression des documents\n",
    "# url = f'http://localhost:9200/{index}/_delete_by_query'\n",
    "\n",
    "# # Corps de la requête de suppression\n",
    "# query = {\n",
    "#     \"query\": {\n",
    "#         \"match_all\": {}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Envoyer la requête de suppression\n",
    "# response = requests.post(url, json=query)\n",
    "\n",
    "# # Vérifier la réponse\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Les documents ont été supprimés avec succès.\")\n",
    "# else:\n",
    "#     print(\"Une erreur s'est produite lors de la suppression des documents.\")\n",
    "#     print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effacer l'index \"notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionTimeout",
     "evalue": "Connection timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionTimeout\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Suppression de l'index \"notes\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnotes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m response \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mdelete(index\u001b[39m=\u001b[39;49mindex_name)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Vérification de la réponse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39macknowledged\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py:690\u001b[0m, in \u001b[0;36mIndicesClient.delete\u001b[0;34m(self, index, allow_no_indices, error_trace, expand_wildcards, filter_path, human, ignore_unavailable, master_timeout, pretty, timeout)\u001b[0m\n\u001b[1;32m    688\u001b[0m     __query[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m timeout\n\u001b[1;32m    689\u001b[0m __headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 690\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mDELETE\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers\n\u001b[1;32m    692\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    390\u001b[0m         method, path, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, body\u001b[39m=\u001b[39;49mbody\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:285\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     target \u001b[39m=\u001b[39m path\n\u001b[0;32m--> 285\u001b[0m meta, resp_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    286\u001b[0m     method,\n\u001b[1;32m    287\u001b[0m     target,\n\u001b[1;32m    288\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    289\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    290\u001b[0m     request_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_timeout,\n\u001b[1;32m    291\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_retries,\n\u001b[1;32m    292\u001b[0m     retry_on_status\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_status,\n\u001b[1;32m    293\u001b[0m     retry_on_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_timeout,\n\u001b[1;32m    294\u001b[0m     client_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client_meta,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[39m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m meta\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    300\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m meta\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m \u001b[39m299\u001b[39m\n\u001b[1;32m    301\u001b[0m     \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elastic_transport/_transport.py:329\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta)\u001b[0m\n\u001b[1;32m    327\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    328\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     meta, raw_data \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    330\u001b[0m         method,\n\u001b[1;32m    331\u001b[0m         target,\n\u001b[1;32m    332\u001b[0m         body\u001b[39m=\u001b[39;49mrequest_body,\n\u001b[1;32m    333\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    334\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m     _logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m [status:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m duration:\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39ms]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py:199\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[0;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[1;32m    191\u001b[0m         err \u001b[39m=\u001b[39m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mstr\u001b[39m(e), errors\u001b[39m=\u001b[39m(e,))\n\u001b[1;32m    192\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(\n\u001b[1;32m    193\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    194\u001b[0m         target\u001b[39m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m         exception\u001b[39m=\u001b[39merr,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mraise\u001b[39;00m err \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    201\u001b[0m meta \u001b[39m=\u001b[39m ApiResponseMeta(\n\u001b[1;32m    202\u001b[0m     node\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m    203\u001b[0m     duration\u001b[39m=\u001b[39mduration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     headers\u001b[39m=\u001b[39mresponse_headers,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(\n\u001b[1;32m    209\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    210\u001b[0m     target\u001b[39m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     response\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    215\u001b[0m )\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m: Connection timed out"
     ]
    }
   ],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "\n",
    "# # Connexion à Elasticsearch\n",
    "# es = Elasticsearch(hosts=['http://localhost:9200'])\n",
    "\n",
    "# # Suppression de l'index \"notes\"\n",
    "# index_name = \"notes\"\n",
    "# response = es.indices.delete(index=index_name)\n",
    "\n",
    "# # Vérification de la réponse\n",
    "# if response['acknowledged']:\n",
    "#     print(f\"L'index '{index_name}' a été supprimé avec succès.\")\n",
    "# else:\n",
    "#     print(f\"Erreur lors de la suppression de l'index '{index_name}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
