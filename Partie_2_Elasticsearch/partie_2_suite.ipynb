{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocker les données avec Elastic Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Questions théoriques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Qu’est ce que le sharding, comment pourrait-on imaginer un sharding sur cet index?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sharding est une technique utilisée dans les systèmes de bases de données distribuées pour **diviser les données en plusieurs fragments appelés shards**. Chaque shard contient une partie des données et est hébergé sur un nœud du cluster. Le sharding permet de **répartir la charge de travail et de stockage de manière équilibrée entre les nœuds**, ce qui permet d'**améliorer les performances et la capacité de traitement** du système.  \n",
    "Lorsque vous avez plusieurs “noeuds” dans un cluster Elastic, chaque Shard est répliqué entre les noeuds pour permettre une résilience lorsqu’un des noeuds est défaillant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe plusieurs approches pour effectuer le sharding :\n",
    "- basé sur une clé de hachage (aléatoire)\n",
    "- basé sur une valeur de champ, ici on peut imaginer sharder en fonction du nom/prénom des patients \n",
    "- basé sur un critère personnalisé, par exemple dans notre cas on pourrait sharder en fonction de la date des documents\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Quelles ingestion de pipelines seraient pertinentes pour notre sujet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ingestion de pipeline fait référence à un ensemble de processus et d'étapes utilisés pour collecter, transformer et charger des données dans un système ou une plateforme. Il s'agit d'une méthodologie structurée qui permet de gérer le flux de données depuis leur source d'origine jusqu'à leur destination finale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de prétraitement du texte :\n",
    "- Nettoyage du texte : Supprimer les caractères spéciaux, la ponctuation, les balises HTML, etc.\n",
    "- Normalisation du texte : Convertir le texte en minuscules, supprimer les accents, etc.\n",
    "- Tokenisation \n",
    "- Suppression des stopwords \n",
    "- Lemmatisation ou racinisation : Réduire les mots à leur forme de base (lemme) ou à leur racine pour normaliser le texte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline d'analyse des sentiments :\n",
    "\n",
    "- Classification des sentiments : Utiliser des techniques de machine learning pour classer les textes en fonction de leurs sentiments (par exemple, \"heureux\", \"triste\", \"colère\", etc.).\n",
    "- Estimation de la confiance : Calculer la confiance ou la probabilité associée à la prédiction des sentiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline d'extraction d'entités :\n",
    "\n",
    "- Reconnaissance des entités nommées : Identifier et extraire des informations spécifiques telles que les noms des patients, les dates, les lieux, etc.\n",
    "- Détection des entités spécifiques aux émotions : Extraire des entités liées aux émotions, telles que les mots clés ou les expressions spécifiques associées à chaque émotion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline d'enrichissement des données :\n",
    "\n",
    "- Analyse de la similarité des textes : Mesurer la similarité entre les textes pour trouver des documents similaires ou des cas de patients similaires.\n",
    "- Extraction de concepts : Identifier et extraire des concepts ou des thématiques spécifiques à partir des textes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Comment aurait-on pu intégrer la gestion des stopwords au niveau du mapping ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour intégrer la gestion des stopwords au niveau du mapping, on peut utiliser l'analyseur de texte personnalisé dans le mapping Elasticsearch. Le code ci-dessous nous définissons un nouvel analyseur appelé \"custom_analyzer\" qui utilise le tokenizer standard et applique 3 filtres : \"lowercase\", \"stop_filter\" et \"punctuation_filter\".  \n",
    "Le champ \"text\" est mis à jour avec l'analyseur \"custom_analyzer\" pour appliquer le filtre de stopwords lors de l'indexation et de la recherche de données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"stop_filter\",\n",
    "            \"punctuation_filter\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"stop_filter\": {\n",
    "          \"type\": \"stop\",\n",
    "          \"stopwords\": \"_english_\"  \n",
    "        },\n",
    "        \"punctuation_filter\": {\n",
    "          \"type\": \"word_delimiter\",\n",
    "          \"generate_word_parts\": false,\n",
    "          \"generate_number_parts\": false,\n",
    "          \"catenate_words\": true,\n",
    "          \"catenate_numbers\": true,\n",
    "          \"catenate_all\": true,\n",
    "          \"split_on_case_change\": true,\n",
    "          \"preserve_original\": true,\n",
    "          \"split_on_numerics\": false,\n",
    "          \"stem_english_possessive\": false,\n",
    "          \"protected_words\": []\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { notre mapping }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comment aurait-on pu intégrer le modèle de ML comme ingest pipeline? (proposez un code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kibana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour télécharger Kibana à partir de Docker dans un conteneur et le connecter à Elastic Search, executez le fichier **bash_commands/set_up_kibana.sh**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour télécharger Kibana à partir du site web et le connecter à notre base de données Elastic Search, suivez les étapes suivantes :\n",
    "- téléchargez Kibana : https://www.elastic.co/fr/downloads/past-releases/kibana-7-17-10  \n",
    "\n",
    "- extrayez de l'archive, puis dans le dossier *config*, ouvrir le fichier *kibana.yml*  \n",
    "- decommentez les lignes `server.host:\"localhost\"` (7) et `elasticsearch.hosts:[\"http://localhost:9200\"]` (32)\n",
    "- depuis le dossier d'installation, dans un terminal de commande, entrez la commande `bin/kibana`\n",
    "- on peut maintenant accéder à kibana à l'adresse : http://localhost:5601.\n",
    "- allez dans dev tools et lancer la requête :  ` GET notes/_search { \"query\": { \"match_all\": {} } }` \n",
    "- allez ensuite dans Discover, créer un index pattern, nommé \"notes\".  \n",
    "- on a maintenant accès aux données indexées avec Elastic Search.     \n",
    "- on peut effectuer des requêtes depuis l'onglet **Discover** et créer des visualisation depuis l'onglet **Dashboard**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
